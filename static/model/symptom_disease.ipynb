{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains a RNN on medical diagnosis of diseases dataset data is obtained from various online sources  \n",
    "Memory network needs to predict the disease using many symptoms listed as natural language sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pdb\n",
    "import h5py\n",
    "import json\n",
    "import string\n",
    "import inflect # convert number into words\n",
    "import pickle\n",
    "import tarfile\n",
    "import itertools\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "max_length = 128  # Maximum length of input sentence to the model.\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "# Labels in our dataset.\n",
    "labels = [\"contradiction\", \"entailment\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[1;31mTimeoutError\u001b[0m: _ssl.c:980: The handshake operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ikath\\OneDrive\\GitHub\\SelfMed\\static\\model\\symptom_disease.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ikath/OneDrive/GitHub/SelfMed/static/model/symptom_disease.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m bert_model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mTFBertModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\modeling_tf_utils.py:2368\u001b[0m, in \u001b[0;36mTFPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   2366\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[0;32m   2367\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[1;32m-> 2368\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[0;32m   2369\u001b[0m         config_path,\n\u001b[0;32m   2370\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[0;32m   2371\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   2372\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[0;32m   2373\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[0;32m   2374\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[0;32m   2375\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2376\u001b[0m         use_auth_token\u001b[39m=\u001b[39muse_auth_token,\n\u001b[0;32m   2377\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[0;32m   2378\u001b[0m         _from_auto\u001b[39m=\u001b[39mfrom_auto_class,\n\u001b[0;32m   2379\u001b[0m         _from_pipeline\u001b[39m=\u001b[39mfrom_pipeline,\n\u001b[0;32m   2380\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[0;32m   2381\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2382\u001b[0m     )\n\u001b[0;32m   2383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2384\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\configuration_utils.py:531\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[0;32m    458\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[1;32m--> 531\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    532\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[0;32m    533\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    536\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\configuration_utils.py:558\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    557\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 558\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[0;32m    560\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\configuration_utils.py:613\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    609\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[0;32m    611\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    612\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 613\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m    614\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    615\u001b[0m         configuration_file,\n\u001b[0;32m    616\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    617\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    618\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    619\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    620\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    621\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    622\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    623\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    624\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m    625\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m    626\u001b[0m     )\n\u001b[0;32m    627\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[0;32m    628\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    406\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    410\u001b[0m         path_or_repo_id,\n\u001b[0;32m    411\u001b[0m         filename,\n\u001b[0;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    425\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    427\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    428\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1053\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1052\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1053\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[0;32m   1054\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1055\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1056\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1057\u001b[0m             timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[0;32m   1058\u001b[0m         )\n\u001b[0;32m   1059\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[0;32m   1060\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(\n\u001b[0;32m   1062\u001b[0m             HUGGINGFACE_HEADER_X_REPO_COMMIT\n\u001b[0;32m   1063\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:1350\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, use_auth_token, proxies, timeout)\u001b[0m\n\u001b[0;32m   1347\u001b[0m headers \u001b[39m=\u001b[39m build_hf_headers(use_auth_token\u001b[39m=\u001b[39muse_auth_token)\n\u001b[0;32m   1349\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1350\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m   1351\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mHEAD\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1352\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m   1353\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m   1354\u001b[0m     allow_redirects\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1355\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   1356\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1357\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m   1358\u001b[0m )\n\u001b[0;32m   1359\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1361\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:398\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 398\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[0;32m    399\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    400\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    401\u001b[0m         max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[0;32m    402\u001b[0m         base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[0;32m    403\u001b[0m         max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[0;32m    404\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    405\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    406\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    407\u001b[0m     )\n\u001b[0;32m    409\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:433\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[0;32m    432\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[0;32m    434\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m    435\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m    436\u001b[0m     max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[0;32m    437\u001b[0m     base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[0;32m    438\u001b[0m     max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[0;32m    439\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39m(ConnectTimeout, ProxyError),\n\u001b[0;32m    440\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39m(),\n\u001b[0;32m    441\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[0;32m    442\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    443\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:105\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m nb_tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[0;32m    107\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\api.py:61\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:542\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    537\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    538\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m: timeout,\n\u001b[0;32m    539\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m'\u001b[39m: allow_redirects,\n\u001b[0;32m    540\u001b[0m }\n\u001b[0;32m    541\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 542\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[0;32m    544\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\sessions.py:655\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    652\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    654\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 655\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    657\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    658\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\adapters.py:439\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    438\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 439\u001b[0m         resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    440\u001b[0m             method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    441\u001b[0m             url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    442\u001b[0m             body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    443\u001b[0m             headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    444\u001b[0m             redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    445\u001b[0m             assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    446\u001b[0m             preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    447\u001b[0m             decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    448\u001b[0m             retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    449\u001b[0m             timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[0;32m    450\u001b[0m         )\n\u001b[0;32m    452\u001b[0m     \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    454\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(conn, \u001b[39m'\u001b[39m\u001b[39mproxy_pool\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    704\u001b[0m     conn,\n\u001b[0;32m    705\u001b[0m     method,\n\u001b[0;32m    706\u001b[0m     url,\n\u001b[0;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    711\u001b[0m )\n\u001b[0;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:386\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    387\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    388\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connectionpool.py:1042\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1042\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1044\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1045\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1046\u001b[0m         (\n\u001b[0;32m   1047\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1052\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1053\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\connection.py:414\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    406\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_certs\n\u001b[0;32m    407\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mca_cert_dir\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(context, \u001b[39m\"\u001b[39m\u001b[39mload_default_certs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    411\u001b[0m ):\n\u001b[0;32m    412\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[1;32m--> 414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[0;32m    415\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[0;32m    416\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[0;32m    417\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[0;32m    418\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[0;32m    419\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[0;32m    420\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[0;32m    421\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[0;32m    422\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    423\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[0;32m    424\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[0;32m    425\u001b[0m )\n\u001b[0;32m    427\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[0;32m    429\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    431\u001b[0m     default_ssl_context\n\u001b[0;32m    432\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    433\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock, \u001b[39m\"\u001b[39m\u001b[39mversion\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    434\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock\u001b[39m.\u001b[39mversion() \u001b[39min\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mTLSv1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTLSv1.1\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m    435\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[1;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[0;32m    437\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn HTTPS request has been made, but the SNI (Server Name \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIndication) extension to TLS is not available on this platform. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    445\u001b[0m         SNIMissingWarning,\n\u001b[0;32m    446\u001b[0m     )\n\u001b[0;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[1;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[0;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[0;32m    451\u001b[0m     )\n\u001b[0;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    453\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(sock, context, tls_in_tls)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\urllib3\\util\\ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[1;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[39mreturn\u001b[39;00m SSLTransport(sock, ssl_context, server_hostname)\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[1;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[0;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39mwrap_socket(sock)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[1;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[0;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[1;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[0;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[0;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[0;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[0;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[0;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[0;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[0;32m    521\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1071\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[1;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m:\n\u001b[0;32m   1069\u001b[0m             \u001b[39m# non-blocking\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1071\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1072\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\ssl.py:1342\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m   1340\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m==\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mand\u001b[39;00m block:\n\u001b[0;32m   1341\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m-> 1342\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[0;32m   1343\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(timeout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('nov1_backup.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " bert (TFBertMainLayer)         TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128, 128)     426496      ['bert[0][0]']                   \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['bidirectional[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_max_pooling1d[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 256)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 3)            771         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,909,507\n",
      "Trainable params: 427,267\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 preprocess text\n",
    "\n",
    "1. Converting to lowercase\n",
    "2. Converting digits to words\n",
    "3. Remove punctuation an whitespace\n",
    "4. Removing default stopwords\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Converting to lowercase\n",
    "Happy > happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Converting digits to words\n",
    "3 > three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_number(text):\n",
    "    p = inflect.engine()\n",
    "    temp_str = text.split()\n",
    "\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove punctuation and whitespace\n",
    "itching   ! > itching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.replace('_', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Removing default stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Lemmatization\n",
    "itching > itch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipe(text):\n",
    "    \"\"\"\n",
    "    Combining all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_word(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"Generates batches of data.\n",
    "\n",
    "    Args:\n",
    "        sentence_pairs: Array of premise and hypothesis input sentences.\n",
    "        labels: Array of labels.\n",
    "        batch_size: Integer batch size.\n",
    "        shuffle: boolean, whether to shuffle the data.\n",
    "        include_targets: boolean, whether to incude the labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuples `([input_ids, attention_mask, `token_type_ids], labels)`\n",
    "        (or just `[input_ids, attention_mask, `token_type_ids]`\n",
    "         if `include_targets=False`)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "\n",
    "        # Loading BERT Tokenizer to encode the text.\n",
    "        # The bert-base-uncased pretrained model is used.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the number of batches per epoch.\"\"\"\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Retrieves the batch of index.\"\"\"\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle indexes after each epoch if shuffle is set to True.\"\"\"\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(42).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_similarity(sentence1, sentence2):\n",
    "    sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "    test_data = BertSemanticDataGenerator(\n",
    "        sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "    )\n",
    "\n",
    "    proba = model.predict(test_data[0])[0]\n",
    "    idx = np.argmax(proba)\n",
    "    proba = f\"{proba[idx]: .2f}%\"\n",
    "    pred = labels[idx]\n",
    "    return pred, proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_disease(dis_dict):\n",
    "    scores = {}\n",
    "    for key, val in tqdm(dis_dict.items(), desc=\"scoring items\"):\n",
    "        scores[key] = check_similarity(val, query)\n",
    "\n",
    "    entails = {}\n",
    "    neutrals = {}\n",
    "    for key, val in scores.items():\n",
    "        if val[0] == 'entailment':\n",
    "            entails[key] = val\n",
    "\n",
    "        if val[0] == 'neutral':\n",
    "            neutrals[key] = val\n",
    "\n",
    "    entails = dict(sorted(entails.items(), key=lambda item:item[1][1], reverse=True))\n",
    "    neutrals = dict(sorted(neutrals.items(), key=lambda item:item[1][1], reverse=True))\n",
    "\n",
    "    if len(entails) == 0:\n",
    "        return list(neutrals.keys())\n",
    "    return list(entails.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../datasets/final data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((46,), (46,))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases = df.disease.values\n",
    "diseases_overview = df.overview.values\n",
    "\n",
    "diseases.shape, diseases_overview.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_dict = {}\n",
    "for i in range(47):\n",
    "    try:\n",
    "        dis_dict[diseases[i]] = preprocess_pipe(diseases_overview[i])\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'feel nauseous fever sore throat'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"i am feeling nauseous and i have fever and sore throat\"\n",
    "query = preprocess_pipe(query)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:   0%|                                    | 0/41 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\ikath\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:   2%|                           | 1/41 [00:13<08:51, 13.28s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:   5%|                          | 2/41 [00:25<08:02, 12.38s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 179ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:   7%|                          | 3/41 [00:29<05:29,  8.68s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 171ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  10%|                         | 4/41 [00:36<05:04,  8.23s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 194ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  12%|                        | 5/41 [00:42<04:19,  7.21s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 174ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  15%|                        | 6/41 [00:49<04:14,  7.27s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 176ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  17%|                       | 7/41 [00:53<03:28,  6.13s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  20%|                      | 8/41 [01:04<04:19,  7.86s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  22%|                     | 9/41 [01:17<04:54,  9.20s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  24%|                    | 10/41 [01:21<03:56,  7.63s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  27%|                   | 11/41 [01:26<03:23,  6.79s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  29%|                   | 12/41 [01:29<02:46,  5.75s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  32%|                  | 13/41 [01:34<02:32,  5.44s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  34%|                 | 14/41 [01:37<02:07,  4.74s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  37%|                 | 15/41 [01:51<03:16,  7.55s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  39%|                | 16/41 [01:57<03:00,  7.23s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  41%|               | 17/41 [01:59<02:15,  5.67s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 161ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  44%|               | 18/41 [02:04<02:02,  5.32s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  46%|              | 19/41 [02:16<02:40,  7.30s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  49%|             | 20/41 [02:28<03:04,  8.78s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  51%|             | 21/41 [02:39<03:09,  9.48s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 172ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  54%|            | 22/41 [02:41<02:19,  7.33s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  56%|           | 23/41 [02:54<02:42,  9.01s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  59%|           | 24/41 [03:01<02:22,  8.40s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 169ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  61%|          | 25/41 [03:06<01:54,  7.14s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  63%|          | 26/41 [03:17<02:04,  8.32s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  66%|         | 27/41 [03:28<02:08,  9.21s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  68%|        | 28/41 [03:32<01:40,  7.73s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  71%|        | 29/41 [03:45<01:52,  9.37s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  73%|       | 30/41 [03:56<01:48,  9.88s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 158ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  76%|      | 31/41 [04:01<01:21,  8.17s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  78%|      | 32/41 [04:07<01:09,  7.73s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  80%|     | 33/41 [04:10<00:49,  6.23s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  83%|    | 34/41 [04:15<00:40,  5.83s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 172ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  85%|    | 35/41 [04:20<00:33,  5.53s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  88%|   | 36/41 [04:28<00:31,  6.34s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  90%|  | 37/41 [04:35<00:25,  6.50s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 188ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  93%|  | 38/41 [04:39<00:17,  5.87s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 157ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  95%| | 39/41 [04:51<00:15,  7.71s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items:  98%|| 40/41 [05:05<00:09,  9.35s/it]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring items: 100%|| 41/41 [05:17<00:00,  7.73s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['common cold',\n",
       " 'constipation',\n",
       " 'dizziness',\n",
       " 'farsightedness',\n",
       " 'myopia',\n",
       " 'acne',\n",
       " 'alopecia',\n",
       " 'baldness',\n",
       " 'sunburn',\n",
       " 'bags under eyes',\n",
       " 'bee sting',\n",
       " 'dandruff',\n",
       " 'ingrown toenails',\n",
       " 'menstrual cramps',\n",
       " 'stings',\n",
       " 'wrist pain',\n",
       " 'body odor and sweating',\n",
       " 'head lice',\n",
       " 'panic attacks and panic disorder',\n",
       " 'acute sinusitis',\n",
       " 'pink eye',\n",
       " 'prickly heat',\n",
       " 'acid reflux',\n",
       " 'cavities',\n",
       " 'hiccups',\n",
       " 'ingrown hair',\n",
       " 'neck pain',\n",
       " 'bad breath',\n",
       " 'diarrhea',\n",
       " 'obesity',\n",
       " 'premenstrual syndrome',\n",
       " 'allergies',\n",
       " 'indigestion',\n",
       " 'calluses and corns',\n",
       " 'corns and calluses',\n",
       " 'snoring',\n",
       " 'polycystic ovary syndrome',\n",
       " 'body lice',\n",
       " 'dengue fever',\n",
       " 'muscle cramp',\n",
       " 'mosquito bites']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_disease(dis_dict)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
