{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trains a RNN on medical diagnosis of diseases dataset data is obtained from various online sources  \n",
    "Memory network needs to predict the disease using many symptoms listed as natural language sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pdb\n",
    "import h5py\n",
    "import json\n",
    "import string\n",
    "import inflect # convert number into words\n",
    "import pickle\n",
    "import tarfile\n",
    "import itertools\n",
    "from functools import reduce\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disease</th>\n",
       "      <th>synonym_disease</th>\n",
       "      <th>synonym_disease2</th>\n",
       "      <th>link</th>\n",
       "      <th>remedies</th>\n",
       "      <th>overview</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acid reflux</td>\n",
       "      <td>gastroesophageal reflux disease</td>\n",
       "      <td>GERD</td>\n",
       "      <td>https://www.mayoclinic.org/diseases-conditions...</td>\n",
       "      <td>Maintain a healthy weight. Excess pounds put p...</td>\n",
       "      <td>Overview.\\n\\nGastroesophageal reflux disease (...</td>\n",
       "      <td>Diagnosis.\\n\\nYour health care provider might ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       disease                  synonym_disease synonym_disease2  \\\n",
       "0  acid reflux  gastroesophageal reflux disease             GERD   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.mayoclinic.org/diseases-conditions...   \n",
       "\n",
       "                                            remedies  \\\n",
       "0  Maintain a healthy weight. Excess pounds put p...   \n",
       "\n",
       "                                            overview  \\\n",
       "0  Overview.\\n\\nGastroesophageal reflux disease (...   \n",
       "\n",
       "                                           treatment  \n",
       "0  Diagnosis.\\n\\nYour health care provider might ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../datasets/updated details 2.xlsx', index_col=0)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47,), (47,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases = df.disease.values\n",
    "diseases_overview = df.overview.values\n",
    "\n",
    "diseases.shape, diseases_overview.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Overview.\\n\\nAcne is a skin condition that occurs when your hair follicles become plugged with oil and dead skin cells. It causes whiteheads, blackheads or pimples. Acne is most common among teenagers, though it affects people of all ages.\\n\\nEffective acne treatments are available, but acne can be persistent. The pimples and bumps heal slowly, and when one begins to go away, others seem to crop up.\\n\\nDepending on its severity, acne can cause emotional distress and scar the skin. The earlier you start treatment, the lower your risk of such problems.\\n\\nSymptoms.\\n\\nAcne signs vary depending on the severity of your condition:\\n\\nWhiteheads (closed plugged pores)\\nBlackheads (open plugged pores)\\nSmall red, tender bumps (papules)\\nPimples (pustules), which are papules with pus at their tips\\nLarge, solid, painful lumps under the skin (nodules)\\nPainful, pus-filled lumps under the skin (cystic lesions).\\n\\nWhiteheads (closed plugged pores).\\n\\nBlackheads (open plugged pores).\\n\\nSmall red, tender bumps (papules).\\n\\nPimples (pustules), which are papules with pus at their tips.\\n\\nLarge, solid, painful lumps under the skin (nodules).\\n\\nPainful, pus-filled lumps under the skin (cystic lesions).\\n\\nAcne usually appears on the face, forehead, chest, upper back and shoulders.\\n\\nIf self-care remedies don't clear your acne, see your primary care doctor. He or she can prescribe stronger medications. If acne persists or is severe, you may want to seek medical treatment from a doctor who specializes in the skin (dermatologist or pediatric dermatologist).\\n\\nFor many women, acne can persist for decades, with flares common a week before menstruation. This type of acne tends to clear up without treatment in women who use contraceptives.\\n\\nIn older adults, a sudden onset of severe acne may signal an underlying disease requiring medical attention.\\n\\nThe Food and Drug Administration (FDA) warns that some popular nonprescription acne lotions, cleansers and other skin products can cause a serious reaction. This type of reaction is quite rare, so don't confuse it with any redness, irritation or itchiness that occurs in areas where you've applied medications or products.\\n\\nSeek emergency medical help if after using a skin product you experience:\\n\\nFaintness\\nDifficulty breathing\\nSwelling of the eyes, face, lips or tongue\\nTightness of the throat.\\n\\nFaintness.\\n\\nDifficulty breathing.\\n\\nSwelling of the eyes, face, lips or tongue.\\n\\nTightness of the throat.\\n\\nFrom Mayo Clinic to your inbox.\\n\\nSign up for free, and stay up to date on research advancements, health tips and current health topics, like COVID-19, plus expertise on managing health.\\n\\nCauses.\\n\\nFour main factors cause acne:\\n\\nExcess oil (sebum) production\\nHair follicles clogged by oil and dead skin cells\\nBacteria\\nInflammation.\\n\\nExcess oil (sebum) production.\\n\\nHair follicles clogged by oil and dead skin cells.\\n\\nBacteria.\\n\\nInflammation.\\n\\nAcne typically appears on your face, forehead, chest, upper back and shoulders because these areas of skin have the most oil (sebaceous) glands. Hair follicles are connected to oil glands.\\n\\nThe follicle wall may bulge and produce a whitehead. Or the plug may be open to the surface and darken, causing a blackhead. A blackhead may look like dirt stuck in pores. But actually the pore is congested with bacteria and oil, which turns brown when it's exposed to the air.\\n\\nPimples are raised red spots with a white center that develop when blocked hair follicles become inflamed or infected with bacteria. Blockages and inflammation deep inside hair follicles produce cystlike lumps beneath the surface of your skin. Other pores in your skin, which are the openings of the sweat glands, aren't usually involved in acne.\\n\\nCertain things may trigger or worsen acne:\\n\\nHormonal changes. Androgens are hormones that increase in boys and girls during puberty and cause the sebaceous glands to enlarge and make more sebum. Hormone changes during midlife, particularly in women, can lead to breakouts too.\\nCertain medications. Examples include drugs containing corticosteroids, testosterone or lithium.\\nDiet. Studies indicate that consuming certain foods — including carbohydrate-rich foods, such as bread, bagels and chips — may worsen acne. Further study is needed to examine whether people with acne would benefit from following specific dietary restrictions.\\nStress. Stress doesn't cause acne, but if you have acne already, stress may make it worse.\\n\\nHormonal changes. Androgens are hormones that increase in boys and girls during puberty and cause the sebaceous glands to enlarge and make more sebum. Hormone changes during midlife, particularly in women, can lead to breakouts too.\\n\\nCertain medications. Examples include drugs containing corticosteroids, testosterone or lithium.\\n\\nDiet. Studies indicate that consuming certain foods — including carbohydrate-rich foods, such as bread, bagels and chips — may worsen acne. Further study is needed to examine whether people with acne would benefit from following specific dietary restrictions.\\n\\nStress. Stress doesn't cause acne, but if you have acne already, stress may make it worse.\\n\\nThese factors have little effect on acne:\\n\\nChocolate and greasy foods. Eating chocolate or greasy food has little to no effect on acne.\\nHygiene. Acne isn't caused by dirty skin. In fact, scrubbing the skin too hard or cleansing with harsh soaps or chemicals irritates the skin and can make acne worse.\\nCosmetics. Cosmetics don't necessarily worsen acne, especially if you use oil-free makeup that doesn't clog pores (noncomedogenics) and remove makeup regularly. Nonoily cosmetics don't interfere with the effectiveness of acne drugs.\\n\\nChocolate and greasy foods. Eating chocolate or greasy food has little to no effect on acne.\\n\\nHygiene. Acne isn't caused by dirty skin. In fact, scrubbing the skin too hard or cleansing with harsh soaps or chemicals irritates the skin and can make acne worse.\\n\\nCosmetics. Cosmetics don't necessarily worsen acne, especially if you use oil-free makeup that doesn't clog pores (noncomedogenics) and remove makeup regularly. Nonoily cosmetics don't interfere with the effectiveness of acne drugs.\\n\\nComplications.\\n\\nPeople with darker skin types are more likely than are people with lighter skin to experience these acne complications:\\n\\nScars. Pitted skin (acne scars) and thick scars (keloids) can remain long-term after acne has healed.\\nSkin changes. After acne has cleared, the affected skin may be darker (hyperpigmented) or lighter (hypopigmented) than before the condition occurred.\\n\\nScars. Pitted skin (acne scars) and thick scars (keloids) can remain long-term after acne has healed.\\n\\nSkin changes. After acne has cleared, the affected skin may be darker (hyperpigmented) or lighter (hypopigmented) than before the condition occurred.\\n\\nRisk factors.\\n\\nRisk factors for acne include:\\n\\nAge. People of all ages can get acne, but it's most common in teenagers.\\nHormonal changes. Such changes are common during puberty or pregnancy.\\nFamily history. Genetics plays a role in acne. If both of your parents had acne, you're likely to develop it too.\\nGreasy or oily substances. You may develop acne where your skin comes into contact with oil or oily lotions and creams.\\nFriction or pressure on your skin. This can be caused by items such as telephones, cellphones, helmets, tight collars and backpacks.\\n\\nAge. People of all ages can get acne, but it's most common in teenagers.\\n\\nHormonal changes. Such changes are common during puberty or pregnancy.\\n\\nFamily history. Genetics plays a role in acne. If both of your parents had acne, you're likely to develop it too.\\n\\nGreasy or oily substances. You may develop acne where your skin comes into contact with oil or oily lotions and creams.\\n\\nFriction or pressure on your skin. This can be caused by items such as telephones, cellphones, helmets, tight collars and backpacks.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diseases_overview[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = {}\n",
    "\n",
    "for i in range(47):\n",
    "    try:\n",
    "        sentence_embeddings[diseases[i]] = model.encode(preprocess_pipe(diseases_overview[i]))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"i have acne\"\n",
    "query_embedding = model.encode(preprocess_pipe(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for key, val in sentence_embeddings.items():\n",
    "    scores[key] = util.cos_sim(query_embedding, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bee sting': tensor([[0.1231]]),\n",
       " 'stings': tensor([[0.1231]]),\n",
       " 'obesity': tensor([[0.1290]]),\n",
       " 'common cold': tensor([[0.1337]]),\n",
       " 'head lice': tensor([[0.1414]]),\n",
       " 'premenstrual syndrome': tensor([[0.1426]]),\n",
       " 'body lice': tensor([[0.1585]]),\n",
       " 'cavities': tensor([[0.1651]]),\n",
       " 'hiccups': tensor([[0.1782]]),\n",
       " 'bad breath': tensor([[0.1870]]),\n",
       " 'allergies': tensor([[0.1901]]),\n",
       " 'snoring': tensor([[0.1913]]),\n",
       " 'panic attacks and panic disorder': tensor([[0.1919]]),\n",
       " 'mosquito bites': tensor([[0.1920]]),\n",
       " 'dengue fever': tensor([[0.1947]]),\n",
       " 'calluses and corns': tensor([[0.2031]]),\n",
       " 'corns and calluses': tensor([[0.2031]]),\n",
       " 'body odor and sweating': tensor([[0.2086]]),\n",
       " 'menstrual cramps': tensor([[0.2160]]),\n",
       " 'polycystic ovary syndrome': tensor([[0.2232]]),\n",
       " 'muscle cramp': tensor([[0.2238]]),\n",
       " 'indigestion': tensor([[0.2269]]),\n",
       " 'constipation': tensor([[0.2320]]),\n",
       " 'acid reflux': tensor([[0.2399]]),\n",
       " 'alopecia': tensor([[0.2409]]),\n",
       " 'baldness': tensor([[0.2409]]),\n",
       " 'myopia': tensor([[0.2464]]),\n",
       " 'dizziness': tensor([[0.2582]]),\n",
       " 'farsightedness': tensor([[0.2582]]),\n",
       " 'ingrown hair': tensor([[0.2672]]),\n",
       " 'sunburn': tensor([[0.2687]]),\n",
       " 'bags under eyes': tensor([[0.2709]]),\n",
       " 'dandruff': tensor([[0.2766]]),\n",
       " 'pink eye': tensor([[0.2851]]),\n",
       " 'neck pain': tensor([[0.2857]]),\n",
       " 'wrist pain': tensor([[0.2913]]),\n",
       " 'acne': tensor([[0.3043]]),\n",
       " 'acute sinusitis': tensor([[0.3141]]),\n",
       " 'prickly heat': tensor([[0.3370]]),\n",
       " 'diarrhea': tensor([[0.3443]]),\n",
       " 'ingrown toenails': tensor([[0.3676]])}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(sorted(scores.items(),key=lambda item:item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 preprocess text\n",
    "\n",
    "1. Converting to lowercase\n",
    "2. Converting digits to words\n",
    "3. Remove punctuation an whitespace\n",
    "4. Removing default stopwords\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Converting to lowercase\n",
    "Happy > happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Converting digits to words\n",
    "3 > three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_number(text):\n",
    "    p = inflect.engine()\n",
    "    temp_str = text.split()\n",
    "\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove punctuation and whitespace\n",
    "itching   ! > itching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.replace('_', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Removing default stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Lemmatization\n",
    "itching > itch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipe(text):\n",
    "    \"\"\"\n",
    "    Combining all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_word(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Preprocessing all symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symp = []\n",
    "disease = []\n",
    "for i in range(len(df_tr)):\n",
    "    symp.append(df_tr.columns[df_tr.iloc[i] == 1].to_list())\n",
    "    disease.append(df_tr.iloc[i, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing all symptoms\n",
    "all_symp = []\n",
    "for sis in symp:\n",
    "    temp = []\n",
    "    for s in sis:\n",
    "        temp.append(preprocess_sym(s))\n",
    "    all_symp.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associates each preprocessed symp with the disease\n",
    "col_dict = dict(zip(disease, all_symp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - syntactic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_set(str1, str2):\n",
    "    list1 = str1.split(' ')\n",
    "    list2 = str2.split(' ')\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syn similarity with the corpus\n",
    "def syntactic_similarity(symp_t, corpus):\n",
    "    most_sim = []\n",
    "    poss_sym = []\n",
    "\n",
    "    for symp in corpus:\n",
    "        d = pattern_set(symp_t, symp)\n",
    "        most_sim.append(d)\n",
    "\n",
    "    order = np.argsort(most_sim)[::-1].tolist()\n",
    "\n",
    "    for i in order:\n",
    "        if DoesExist(symp_t):\n",
    "            return 1, [corpus[i]]\n",
    "\n",
    "        if corpus[i] not in poss_sym and most_sim[i] != 0:\n",
    "            poss_sym.append(corpus[i])\n",
    "\n",
    "    if len(poss_sym):\n",
    "        return 1, poss_sym\n",
    "\n",
    "\n",
    "    else:\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns all the subsets of this set. This is a generator.\n",
    "def powerset(seq):\n",
    "    if len(seq) <= 1:\n",
    "        yield seq\n",
    "        yield []\n",
    "    else:\n",
    "        for item in powerset(seq[1:]):\n",
    "            yield [seq[0]]+item\n",
    "            yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort list based on length\n",
    "def sort(a):\n",
    "    for i in range(len(a)):\n",
    "        for j in range(i+1,len(a)):\n",
    "            if len(a[j])>len(a[i]):\n",
    "                a[i],a[j]=a[j],a[i]\n",
    "    a.pop()\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all permutations of a list\n",
    "def permutations(s):\n",
    "    permutations = list(itertools.permutations(s))\n",
    "    return([' '.join(permutation) for permutation in permutations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DoesExist(txt):\n",
    "    txt=txt.split(' ')\n",
    "    combinations = [x for x in powerset(txt)]\n",
    "    sort(combinations)\n",
    "\n",
    "    for comb in combinations :\n",
    "        # print(permutations(comb))\n",
    "        for sym in permutations(comb):\n",
    "            if sym in all_symp_pr:\n",
    "                # print(sym)\n",
    "                return sym\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DoesExist('worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_sym('really worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syntactic_similarity(preprocess_sym('nervous'), all_symp_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pattern(inp,dis_list):\n",
    "    import re\n",
    "    pred_list=[]\n",
    "    ptr=0\n",
    "    patt = \"^\" + inp + \"$\"\n",
    "    regexp = re.compile(inp)\n",
    "    for item in dis_list:\n",
    "        if regexp.search(item):\n",
    "            pred_list.append(item)\n",
    "    if(len(pred_list)>0):\n",
    "        return 1,pred_list\n",
    "    else:\n",
    "        return ptr,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pattern('nail', all_symp_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - semantic similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sentence1, sentence2):\n",
    "    # encode list of sentences to get their embeddings\n",
    "    embedding1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "    embedding2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "    # compute similarity scores of two embeddings\n",
    "    cosine_scores = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_similarity('tensed', 'nervous')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity(symp_t, corpus):\n",
    "    sims = {}\n",
    "    for symp in corpus:\n",
    "        d = sentence_similarity(symp_t, symp)\n",
    "        sims[symp] = d\n",
    "\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_pr.sort()\n",
    "semantic_similarity('tensed', all_symp_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symp_pr.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSD('season', 'apply spices to the chicken to season it').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_symp_pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_syn(sym):\n",
    "    symp = []\n",
    "    synonyms = wordnet.synsets(sym)\n",
    "    lemmas=[word.lemma_names() for word in synonyms]\n",
    "    lemmas = list(set(chain(*lemmas)))\n",
    "    for e in lemmas:\n",
    "        res,sym1=semantic_similarity(e,all_symp_pr)\n",
    "        if res != 0:\n",
    "            symp.append(sym1)\n",
    "    return list(set(symp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_syn('worried')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recoit client_symptoms et renvoit un dataframe avec 1 pour les symptoms associees\n",
    "def OHV(cl_sym,all_sym):\n",
    "    l=np.zeros([1,len(all_sym)])\n",
    "    for sym in cl_sym:\n",
    "        l[0,all_sym.index(sym)]=1\n",
    "    return pd.DataFrame(l, columns =all_symp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains(small, big):\n",
    "    a=True\n",
    "    for i in small:\n",
    "        if i not in big:\n",
    "            a=False\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possible_diseases(l):\n",
    "    poss_dis=[]\n",
    "    for dis in set(disease):\n",
    "        if contains(l,symVONdisease(df_tr,dis)):\n",
    "            poss_dis.append(dis)\n",
    "    return poss_dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(disease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recoit une maladie renvoit tous les sympts\n",
    "def symVONdisease(df,disease):\n",
    "    ddf=df[df.prognosis==disease]\n",
    "    m2 = (ddf == 1).any()\n",
    "    return m2.index[m2].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symVONdisease(df_tr,'Jaundice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_repeated(name,repeated_list):\n",
    "    name = name.lower().strip()\n",
    "    return name if not (name in repeated_list) else repeated_list[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_title(word):\n",
    "    return re.sub(r'\\W+', ' ', word).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip().lower() for x in re.split('(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_stories(lines, only_supporting=False,repeated_list=None):\n",
    "    '''Parse stories\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        spl = line.split(' ', 1)\n",
    "        if len(spl) > 1:\n",
    "            nid, line = spl\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            nid = int(nid)\n",
    "        except ValueError:\n",
    "            pdb.set_trace()\n",
    "        if nid == 0:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            supporting, a = line.split('\\t')\n",
    "            a = map(process_title,a.split(','))\n",
    "            options = [] if len(a) == 1 else list(set(a[1:]))\n",
    "            a = a[0]    \n",
    "            substory = None\n",
    "            # Provide all the substories\n",
    "            if supporting:\n",
    "                story.append([tokenize(supporting) + [u'.']])\n",
    "            substory = [x for x in story if x]\n",
    "            # TODO: I should have done the lower in previous processing steps\n",
    "            if not substory:\n",
    "                continue\n",
    "            data.append((substory, a.lower(), map(lambda x:x.lower(),options)))\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append([sent + [u'.']])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stories(f, only_supporting=False, max_length=None,repeated_list=None, min_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting,repeated_list=repeated_list)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [[flatten(reversed(story)), answer, options] for story,answer,options in data if not max_length or len(flatten(story)) < max_length]\n",
    "    # At least two facts\n",
    "    print(len(data))\n",
    "    if min_length: \n",
    "        data = filter(lambda x: len(x[0]) > min_length, data)\n",
    "    print(len(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    X = []\n",
    "    Xq = []\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        y = np.zeros(len(word_idx))  # let's not forget that index 0 is reserved\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen),\n",
    "            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spacy_vectors(data, answer_dict, story_maxlen, model):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for story,answer in data:\n",
    "        story = story[:story_maxlen] if len(story) > story_maxlen else story\n",
    "        x = [model(unicode(w)).vector for w in story]\n",
    "        X.append(x)\n",
    "        if not answer_dict is None:\n",
    "            y = np.zeros(len(answer_dict))\n",
    "            y[answer_dict[answer]] = 1\n",
    "            Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen,dtype='float32'),\n",
    "            np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vectors(data, answer_dict, story_maxlen, model):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for story,answer in data:\n",
    "        story = story[:story_maxlen] if len(story) > story_maxlen else story\n",
    "        x = [model.word_vectors[model.dictionary[w]] for w in story if \n",
    "             not model.dictionary.get(w) is None]\n",
    "        X.append(x)\n",
    "        if not answer_dict is None:\n",
    "            y = np.zeros(len(answer_dict))\n",
    "            y[answer_dict[answer]] = 1\n",
    "            Y.append(y)\n",
    "    return (pad_sequences(X, maxlen=story_maxlen,dtype='float32'),\n",
    "            np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors_dataset(input_files, vector_files, max_len=500):\n",
    "    print('Creating word vectors file')\n",
    "\n",
    "    training_set_file, test_set_file = input_files\n",
    "    train_word_file, test_word_file = vector_files\n",
    "    \n",
    "    train_stories = pickle.load(open(training_set_file,'r'))\n",
    "    test_stories = pickle.load(open(test_set_file,'r'))\n",
    "\n",
    "    train_stories = [(reduce(lambda x,y: x + y, map(list,fact)),q) for fact,q in train_stories]\n",
    "    test_stories = [(reduce(lambda x,y: x + y, map(list,fact)),q) for fact,q in test_stories]\n",
    "\n",
    "    vocab = sorted(reduce(lambda x, y: x | y, (set(story + [answer]) for story, answer in train_stories + test_stories)))\n",
    "\n",
    "    # Reserve 0 for masking via pad_sequences\n",
    "    vocab_size = len(vocab) + 1\n",
    "    story_maxlen = max(map(len, (x for x, _ in train_stories + test_stories)))\n",
    "\n",
    "\n",
    "    print('-')\n",
    "    print('Vocab size:', vocab_size, 'unique words')\n",
    "    print('Story max length:', story_maxlen, 'words')\n",
    "    print('Number of training stories:', len(train_stories))\n",
    "    print('Number of test stories:', len(test_stories))\n",
    "    print('-')\n",
    "    print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n",
    "    print(train_stories[0])\n",
    "    print('-')\n",
    "    print('Vectorizing the word sequences...')\n",
    "\n",
    "    word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "\n",
    "    answer_vocab = sorted(reduce(lambda x, y: x | y, (set([answer]) for _, answer in train_stories + test_stories)))\n",
    "    # Reserve 0 for masking via pad_sequences\n",
    "    answer_dict = dict((word, i) for i, word in enumerate(answer_vocab))\n",
    "    print('Answers dict len: {0}'.format(len(answer_dict)))\n",
    "\n",
    "    # I need to check also if this exist\n",
    "    word_vectors_dir = 'word_vectors/glove.42B.300d.txt'\n",
    "    word_vectors_model = Glove.load_stanford(word_vectors_dir)\n",
    "\n",
    "    inputs_train, answers_train = get_word_vectors(train_stories, answer_dict, \n",
    "                                                   max_len, word_vectors_model)\n",
    "    inputs_test, answers_test = get_word_vectors(test_stories, answer_dict, max_len,\n",
    "                                                 word_vectors_model)\n",
    "\n",
    "    with h5py.File(train_word_file,'w') as train_f:\n",
    "        _ = train_f.create_dataset('inputs',data=inputs_train)\n",
    "        _ = train_f.create_dataset('answers',data=answers_train)\n",
    "    with h5py.File(test_word_file,'w') as test_f:\n",
    "        _ = test_f.create_dataset('inputs',data=inputs_test)\n",
    "        _ = test_f.create_dataset('answers',data=answers_test)\n",
    "        \n",
    "    return (inputs_train, answers_train),(inputs_test, answers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vectors_dict(input_files):\n",
    "    # I need to check also if this exist\n",
    "    filename = 'word_vectors/glove.42B.300d.txt'\n",
    "    word_vectors_dict = 'word_vectors/glove_dict.hdf5'\n",
    "    dct = {}\n",
    "    vectors = array.array('d')\n",
    "\n",
    "    # Read in the data.\n",
    "    with io.open(filename, 'r', encoding='utf-8') as savefile:\n",
    "        for i, line in enumerate(savefile):\n",
    "            tokens = line.split(' ')\n",
    "\n",
    "            word = tokens[0]\n",
    "            entries = tokens[1:]\n",
    "\n",
    "            dct[word] = i\n",
    "            vectors.extend(float(x) for x in entries)\n",
    "            \n",
    "    print('Saving to hf5 file')\n",
    "    with h5py.File(word_vectors_dict,'w') as vector_f:\n",
    "        _ = vector_f.create_dataset('vectors',data=dct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN = recurrent.LSTM\n",
    "NUM_HIDDEN_UNITS = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "DROPOUT_FACTOR = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "word_vec_dim = 300\n",
    "vocab_size = 2350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_file = 'data/training_set.dat'\n",
    "test_set_file = 'data/test_set.dat'\n",
    "\n",
    "train_stories = pickle.load(open(training_set_file,'r'))\n",
    "test_stories = pickle.load(open(test_set_file,'r'))\n",
    "\n",
    "train_stories = [(reduce(lambda x,y: x + y, map(list,fact)),q) for fact,q in train_stories]\n",
    "test_stories = [(reduce(lambda x,y: x + y, map(list,fact)),q) for fact,q in test_stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_vocab = sorted(reduce(lambda x, y: x | y, (set([answer]) for _, answer in train_stories + test_stories)))\n",
    "\n",
    "# Reserve 0 for masking via pad_sequences\n",
    "answer_dict = dict((word, i) for i, word in enumerate(answer_vocab))\n",
    "print('Answers dict len: {0}'.format(len(answer_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to check also if this exist\n",
    "# word_vectors_dir = 'word_vectors/glove.42B.300d.txt'\n",
    "# word_vectors_model = Glove.load_stanford(word_vectors_dir)\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# GRU\n",
    "model.add(GRU(\n",
    "    output_dim=NUM_HIDDEN_UNITS,\n",
    "    activation='tanh',\n",
    "    return_sequences=True,\n",
    "    input_shape=(max_len, word_vec_dim)\n",
    "))\n",
    "\n",
    "# Dropout\n",
    "model.add(Dropout(DROPOUT_FACTOR))\n",
    "\n",
    "# GRU\n",
    "model.add(GRU(\n",
    "    NUM_HIDDEN_UNITS,\n",
    "    return_sequences=False\n",
    "))\n",
    "\n",
    "# Dense\n",
    "model.add(Dense(\n",
    "    vocab_size,\n",
    "    init='uniform',\n",
    "    activation='softmax'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '.'\n",
    "\n",
    "NUM_DATA_TRAIN = len(train_stories)\n",
    "NUM_DATA_TEST = len(test_stories)\n",
    "\n",
    "random.shuffle(train_stories)\n",
    "valid_stories = train_stories[int(len(train_stories)*0.95):]\n",
    "train_stories = train_stories[:int(len(train_stories)*0.95)]\n",
    "\n",
    "print('Validation size: {0}'.format(len(valid_stories)))\n",
    "print('Training size: {0}'.format(len(train_stories)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouper(iterable, n, fillvalue=None):\n",
    "    args = [iter(iterable)] * n\n",
    "    return izip_longest(*args, fillvalue=fillvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_hist = []\n",
    "acc_hist.append(0.)\n",
    "\n",
    "show_batch_interval = 50\n",
    "\n",
    "for k in xrange(EPOCHS):\n",
    "    for b,train_batch in enumerate(zip(grouper(train_stories, BATCH_SIZE, fillvalue=train_stories[-1]))):\n",
    "        X,Y = get_spacy_vectors(train_batch[0], answer_dict, \n",
    "                                         max_len, nlp)\n",
    "            \n",
    "        loss = model.train_on_batch(X, Y)\n",
    "        if b % show_batch_interval == 0:\n",
    "            print('Epoch: {0}, Batch: {1}, loss: {2}'.format(k,b,loss))\n",
    "    \n",
    "    X,Y = get_spacy_vectors(valid_stories, answer_dict, \n",
    "                                        max_len, nlp)\n",
    "    loss, acc = model.evaluate(X, Y, batch_size=BATCH_SIZE)\n",
    "    print('Epoch{0}, Valid loss / valid accuracy = {1:.4f} / {2:.4f}'.format(k,loss, acc))\n",
    "\n",
    "    # logging results\n",
    "    with open(base_dir + '/logs/log_{0}_{1}_drop_{2}.txt'.format(\n",
    "        'GRU',str(NUM_HIDDEN_UNITS),str(DROPOUT_FACTOR)),'a') as fil:\n",
    "        fil.write(str(loss) + ' ' + str(acc) + '\\n')\n",
    "\n",
    "    # saving model\n",
    "    if max(acc_hist) < acc:\n",
    "        model.save_weights(base_dir + '/models/weights_{0}_{1}_drop_{2}.hdf5'.format(\n",
    "                'GRU',str(NUM_HIDDEN_UNITS),str(DROPOUT_FACTOR)),overwrite=True)\n",
    "\n",
    "    acc_hist.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_5 = 0.\n",
    "acc = 0.\n",
    "\n",
    "for b,test_batch in enumerate(zip(grouper(test_stories, BATCH_SIZE, fillvalue=test_stories[-1]))):\n",
    "    X,Y = get_spacy_vectors(\n",
    "        test_batch[0], answer_dict, max_len, nlp\n",
    "    )\n",
    "    answers_test = Y if b == 0 else np.vstack((answers_test,Y))\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    # Saving in order to make some more visualizations\n",
    "    all_predictions = preds if b == 0 else np.vstack((all_predictions,preds))\n",
    "    if b % 50 == 0:\n",
    "        print('Batch: {0}'.format(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = all_predictions[:len(test_stories)]\n",
    "answers_test = answers_test[:len(test_stories)]\n",
    "\n",
    "for k,(pred,answer) in enumerate(zip(all_predictions, answers_test)):\n",
    "    prediction = np.argsort(pred)[-5:][::-1]\n",
    "    pred_words = [answer_dict.keys()[answer_dict.values().index(pred)] for pred in prediction]\n",
    "    answer_word = answer_dict.keys()[answer_dict.values().index(answer.argmax())]\n",
    "\n",
    "    if answer_word in pred_words:\n",
    "        acc_5 += 1.\n",
    "\n",
    "    if pred_words[0] == answer_word:\n",
    "        acc += 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_err = -np.log(all_predictions[range(all_predictions.shape[0]),answers_test.argmax(axis=1)])\n",
    "\n",
    "np.savetxt('logs/error.dat', all_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc /= len(test_stories)\n",
    "acc_5 /= len(test_stories)\n",
    "\n",
    "print('Accuracy: {0}'.format(acc))\n",
    "print('5 most prob. accuracy: {0}'.format(acc_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  VI- SEVERITY / DESCRIPTION / PRECAUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severityDictionary=dict()\n",
    "description_list = dict()\n",
    "precautionDictionary=dict()\n",
    "\n",
    "def getDescription():\n",
    "    global description_list\n",
    "    with open(DATASETS_DIR + 'symptom_Description.csv') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            _description={row[0]:row[1]}\n",
    "            description_list.update(_description)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getSeverityDict():\n",
    "    global severityDictionary\n",
    "    with open(DATASETS_DIR + 'symptom_severity.csv') as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        try:\n",
    "            for row in csv_reader:\n",
    "                _diction={row[0]:int(row[1])}\n",
    "                severityDictionary.update(_diction)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def getprecautionDict():\n",
    "    global precautionDictionary\n",
    "    with open(DATASETS_DIR + 'symptom_precaution.csv') as csv_file:\n",
    "\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            _prec={row[0]:[row[1],row[2],row[3],row[4]]}\n",
    "            precautionDictionary.update(_prec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSeverityDict()\n",
    "getprecautionDict()\n",
    "getDescription()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "severityDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_condition(exp,days):\n",
    "    sum=0\n",
    "    for item in exp:\n",
    "        if item in severityDictionary.keys():\n",
    "            sum=sum+severityDictionary[item]\n",
    "    if((sum*days)/(len(exp))>13):\n",
    "        return 1\n",
    "        print(\"You should take the consultation from doctor. \")\n",
    "    else:\n",
    "        return 0\n",
    "        print(\"It might not be that bad but you should take precautions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfo():\n",
    "    # name=input(\"Name:\")\n",
    "    print(\"Your Name \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    name=input(\"\")\n",
    "    print(\"hello \",name)\n",
    "    return str(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def related_sym(psym1):\n",
    "    if len(psym1)==1:\n",
    "        return psym1[0]\n",
    "    print(\"searches related to input: \")\n",
    "    for num,it in enumerate(psym1):\n",
    "        print(num,\")\",clean_symp(it))\n",
    "    if num!=0:\n",
    "        print(f\"Select the one you meant (0 - {num}):  \", end=\"\")\n",
    "        conf_inp = int(input(\"\"))\n",
    "    else:\n",
    "        conf_inp=0\n",
    "\n",
    "    disease_input=psym1[conf_inp]\n",
    "    return disease_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_sp(name,all_symp_col):\n",
    "    #main Idea: At least two initial sympts to start with\n",
    "    \n",
    "    #get the 1st syp ->> process it ->> check_pattern ->>> get the appropriate one (if check_pattern==1 == similar syntaxic symp found)\n",
    "    print(\"Enter the main symptom you are experiencing Mr/Ms \"+name+\"  \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    sym1 = input(\"\")\n",
    "    sym1=preprocess_sym(sym1)\n",
    "    sim1,psym1=syntactic_similarity(sym1,all_symp_pr)\n",
    "    if sim1==1:\n",
    "        psym1=related_sym(psym1)\n",
    "    \n",
    "    #get the 2nd syp ->> process it ->> check_pattern ->>> get the appropriate one (if check_pattern==1 == similar syntaxic symp found)\n",
    "\n",
    "    print(\"Enter a second symptom you are experiencing Mr/Ms \"+name+\"  \\n\\t\\t\\t\\t\\t\\t\",end=\"=>\")\n",
    "    sym2=input(\"\")\n",
    "    sym2=preprocess_sym(sym2)\n",
    "    sim2,psym2=syntactic_similarity(sym2,all_symp_pr)\n",
    "    if sim2==1:\n",
    "        psym2=related_sym(psym2)\n",
    "        \n",
    "    #if check_pattern==0 no similar syntaxic symp1 or symp2 ->> try semantic similarity\n",
    "    \n",
    "    if sim1==0 or sim2==0:\n",
    "        sim1,psym1=semantic_similarity(sym1,all_symp_pr)\n",
    "        sim2,psym2=semantic_similarity(sym2,all_symp_pr)\n",
    "        \n",
    "        #if semantic sim syp1 ==0 (no symp found) ->> suggest possible data symptoms based on all data and input sym synonymes\n",
    "        if sim1==0:\n",
    "            sugg=suggest_syn(sym1)\n",
    "            print('Are you experiencing any ')\n",
    "            for res in sugg:\n",
    "                print(res)\n",
    "                inp=input('')\n",
    "                if inp==\"yes\":\n",
    "                    psym1=res\n",
    "                    sim1=1\n",
    "                    break\n",
    "                \n",
    "        #if semantic sim syp2 ==0 (no symp found) ->> suggest possible data symptoms based on all data and input sym synonymes\n",
    "        if sim2==0:\n",
    "            sugg=suggest_syn(sym2)\n",
    "            for res in sugg:\n",
    "                inp=input('Do you feel '+ res+\" ?(yes or no) \")\n",
    "                if inp==\"yes\":\n",
    "                    psym2=res\n",
    "                    sim2=1\n",
    "                    break\n",
    "        #if no syntaxic semantic and suggested sym found return None and ask for clarification\n",
    "\n",
    "        if sim1==0 and sim2==0:\n",
    "            return None,None\n",
    "        else:\n",
    "            # if at least one sym found ->> duplicate it and proceed\n",
    "            if sim1==0:\n",
    "                psym1=psym2\n",
    "            if sim2==0:\n",
    "                psym2=psym1\n",
    "    #create patient symp list\n",
    "    all_sym=[col_dict[psym1],col_dict[psym2]]\n",
    "    #predict possible diseases\n",
    "    diseases=possible_diseases(all_sym)\n",
    "    stop=False\n",
    "    print(\"Are you experiencing any \")\n",
    "    for dis in diseases:\n",
    "        print(diseases)\n",
    "        if stop==False:\n",
    "            for sym in symVONdisease(df_tr,dis):\n",
    "                if sym not in all_sym:\n",
    "                    print(clean_symp(sym)+' ?')\n",
    "                    while True:\n",
    "                        inp=input(\"\")\n",
    "                        if(inp==\"yes\" or inp==\"no\"):\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"provide proper answers i.e. (yes/no) : \",end=\"\")\n",
    "                    if inp==\"yes\":\n",
    "                        all_sym.append(sym)\n",
    "                        diseases=possible_diseases(all_sym)\n",
    "                        if len(diseases)==1:\n",
    "                            stop=True \n",
    "    return knn_clf.predict(OHV(all_sym,all_symp_col)),all_sym\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_sp():\n",
    "    a=True\n",
    "    while a:\n",
    "        name=getInfo()\n",
    "        result,sym=main_sp(name,all_symp_col)\n",
    "        if result == None :\n",
    "            ans3=input(\"can you specify more what you feel or tap q to stop the conversation\")\n",
    "            if ans3==\"q\":\n",
    "                a=False\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        else:\n",
    "            print(\"you may have \"+result[0])\n",
    "            print(description_list[result[0]])\n",
    "            an=input(\"how many day do you feel those symptoms ?\")\n",
    "            if calc_condition(sym,int(an))==1:\n",
    "                print(\"you should take the consultation from doctor\")\n",
    "            else : \n",
    "                print('Take following precautions : ')\n",
    "                for e in precautionDictionary[result[0]]:\n",
    "                    print(e)\n",
    "            print(\"do you need another medical consultation (yes or no)? \")\n",
    "            ans=input()\n",
    "            if ans!=\"yes\":\n",
    "                a=False\n",
    "                print(\"!!!!! thanks for using ower application !!!!!! \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "# knn_clf = joblib.load('model/knn.pkl')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symVONdisease(df_tr,\"Jaundice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.predict(OHV(['fatigue', 'weight_loss', 'itching','high_fever'],all_symp_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=df_tr[df_tr.iloc[:,-1]==\"Fungal infection\"].sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl=df_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp=d!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl[pp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[pp].drop('prognosis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chat_sp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
