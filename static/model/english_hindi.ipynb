{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "https://www.sbert.net/examples/applications/semantic-search/README.html\n",
    "\n",
    "https://www.kaggle.com/datasets/mathurinache/samanantar\n",
    "\n",
    "https://www.kaggle.com/datasets/eshuenglish/semantic-similarity?select=cw2_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, LoggingHandler, models, evaluation, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers.datasets import ParallelSentencesDataset\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import sentence_transformers.util\n",
    "import csv\n",
    "import gzip\n",
    "from tqdm.autonotebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our monolingual teacher model, we want to convert to multiple languages\n",
    "teacher_model_name = 'paraphrase-distilroberta-base-v2'\n",
    "\n",
    "# Multilingual base model we use to imitate the teacher model\n",
    "student_model_name = 'xlm-roberta-base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128                #Student model max. lengths for inputs (number of word pieces)\n",
    "train_batch_size = 64               #Batch size for training\n",
    "inference_batch_size = 64           #Batch size at inference\n",
    "max_sentences_per_language = 500000 #Maximum number of  parallel sentences for training\n",
    "train_max_sentence_length = 250     #Maximum length (characters) for parallel training sentences\n",
    "\n",
    "num_epochs = 5                       #Train for x epochs\n",
    "num_warmup_steps = 10000             #Warumup steps\n",
    "\n",
    "num_evaluation_steps = 1000          #Evaluate performance after every xxxx steps\n",
    "dev_sentences = 1000                 #Number of parallel sentences to be used for development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_sentences_folder = \"../datasets/english-hindi/\"\n",
    "\n",
    "# Create parallel files for the selected language combinations\n",
    "os.makedirs(parallel_sentences_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Start the extension of the teacher model to multiple languages ########\n",
    "logger.info(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer(teacher_model_name)\n",
    "\n",
    "\n",
    "logger.info(\"Create student model from scratch\")\n",
    "word_embedding_model = models.Transformer(student_model_name, max_seq_length=max_seq_length)\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "student_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Read Parallel Sentences Dataset ######\n",
    "train_data = ParallelSentencesDataset(\n",
    "    student_model=student_model,\n",
    "    teacher_model=teacher_model,\n",
    "    batch_size=inference_batch_size,\n",
    "    use_embedding_cache=True\n",
    ")\n",
    "\n",
    "train_data.load_data(\n",
    "    '../datasets/english-hindi/train.csv',\n",
    "    max_sentences=max_sentences_per_language,\n",
    "    max_sentence_length=train_max_sentence_length\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=student_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Evaluate cross-lingual performance on different tasks #####\n",
    "evaluators = []  # evaluators has a list of different evaluator classes we call periodically\n",
    "v = pd.read_csv('../datasets/english-hindi/test.csv')\n",
    "src_sentences = v.Sent2.values\n",
    "trg_sentences = v.Sent1.values\n",
    "\n",
    "\n",
    "# Mean Squared Error (MSE) measures the (euclidean) distance between teacher and student embeddings\n",
    "dev_mse = evaluation.MSEEvaluator(\n",
    "    src_sentences,\n",
    "    trg_sentences,\n",
    "    name='dev',\n",
    "    teacher_model=teacher_model,\n",
    "    batch_size=inference_batch_size\n",
    ")\n",
    "evaluators.append(dev_mse)\n",
    "\n",
    "# TranslationEvaluator computes the embeddings for all parallel sentences. It then check if the embedding of source[i] is the closest to target[i] out of all available target sentences\n",
    "dev_trans_acc = evaluation.TranslationEvaluator(\n",
    "    src_sentences, trg_sentences,\n",
    "    name='dev',\n",
    "    batch_size=inference_batch_size\n",
    ")\n",
    "evaluators.append(dev_trans_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.read_csv('../datasets/english-hindi/dev.csv')\n",
    "dev_src = d.Sent2.values\n",
    "dev_trg = d.Sent1.values\n",
    "dev_scores = d.SimScore.values\n",
    "\n",
    "dev_src.shape, dev_trg.shape, dev_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    dev_src, dev_trg, dev_scores,\n",
    "    batch_size=inference_batch_size,\n",
    "    name='test',\n",
    "    show_progress_bar=False\n",
    ")\n",
    "evaluators.append(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"output/make-multilingual-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Train the model\n",
    "student_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator(\n",
    "        evaluators, main_score_function=lambda scores: np.mean(scores)\n",
    "    ),\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=num_warmup_steps,\n",
    "    evaluation_steps=num_evaluation_steps,\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params={'lr': 2e-5, 'eps': 1e-6,}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create teacher and student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CREATE MODEL ######\n",
    "max_seq_length = 128\n",
    "train_batch_size = 64\n",
    "\n",
    "# Load teacher model\n",
    "print(\"Load teacher model\")\n",
    "teacher_model = SentenceTransformer('bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# Create student model\n",
    "print(\"Create student model\")\n",
    "word_embedding_model = models.Transformer(\"xlm-roberta-base\")\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('../datasets/english-hindi/train.csv')\n",
    "d = pd.read_csv('../datasets/english-hindi/dev.csv')\n",
    "v = pd.read_csv('../datasets/english-hindi/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_src = d.Sent1.values\n",
    "dev_trg = d.Sent2.values\n",
    "dev_scores = d.SimScore.values\n",
    "\n",
    "dev_src.shape, dev_trg.shape, dev_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = v.Sent1.values\n",
    "test_trg = v.Sent2.values\n",
    "\n",
    "test_src.shape, test_trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Load train sets ######\n",
    "\n",
    "train_reader = ParallelSentencesDataset(student_model=model, teacher_model=teacher_model)\n",
    "train_reader.load_data('../datasets/english-hindi/train.txt')\n",
    "train_dataloader = DataLoader(train_reader, shuffle=True, batch_size=train_batch_size)\n",
    "train_loss = losses.MSELoss(model=model)\n",
    "\n",
    "\n",
    "###### Load dev sets ######\n",
    "\n",
    "evaluators = []\n",
    "# sts_reader = readers.STSDataReader('../datasets/english-hindi/', s1_col_idx=0, s2_col_idx=1, score_col_idx=2)\n",
    "# dev_data = SentencesDataset(examples=sts_reader.get_examples('dev.txt'), model=model)\n",
    "# dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
    "evaluator_sts = evaluation.EmbeddingSimilarityEvaluator(\n",
    "    dev_src, dev_trg, dev_scores, name='dev'\n",
    ")\n",
    "evaluators.append(evaluator_sts)\n",
    "\n",
    "\n",
    "###### Load test sets ######\n",
    "\n",
    "# test_reader = ParallelSentencesDataset(student_model=model, teacher_model=teacher_model)\n",
    "# test_reader.load_data('../datasets/english-hindi/test.txt')\n",
    "# test_dataloader = DataLoader(test_reader, shuffle=False, batch_size=train_batch_size)\n",
    "test_mse = evaluation.MSEEvaluator(\n",
    "    test_src, test_trg, name='test',\n",
    "    teacher_model=teacher_model\n",
    ")\n",
    "evaluators.append(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "<!-- \n",
    "output_path = \"output/model-\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1]),\n",
    "    epochs=20,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=10000,\n",
    "    scheduler='warmupconstant',\n",
    "    output_path=output_path,\n",
    "    save_best_model=True,\n",
    "    optimizer_params= {'lr': 2e-5, 'eps': 1e-6, 'correct_bias': False}\n",
    ")\n",
    " -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_path = \"./output/model-\" + datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "model.fit(\n",
    "    train_objectives = [(train_dataloader, train_loss)],\n",
    "    evaluator = evaluation.SequentialEvaluator(evaluators, main_score_function=lambda scores: scores[-1]),\n",
    "    epochs = 20,\n",
    "    evaluation_steps = 1000,\n",
    "    warmup_steps = 1000,\n",
    "    scheduler = 'warmupconstant',\n",
    "    output_path = output_path,\n",
    "    save_best_model = True,\n",
    "    optimizer_params = {'lr': 2e-5, 'eps': 1e-6}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 preprocess text\n",
    "\n",
    "1. Converting to lowercase\n",
    "2. Converting digits to words\n",
    "3. Remove punctuation an whitespace\n",
    "4. Removing default stopwords\n",
    "5. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Converting to lowercase\n",
    "Happy > happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Converting digits to words\n",
    "3 > three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_number(text):\n",
    "    p = inflect.engine()\n",
    "    temp_str = text.split()\n",
    "\n",
    "    new_string = []\n",
    "\n",
    "    for word in temp_str:\n",
    "        if word.isdigit():\n",
    "            temp = p.number_to_words(word)\n",
    "            new_string.append(temp)\n",
    "\n",
    "        else:\n",
    "            new_string.append(word)\n",
    "\n",
    "    temp_str = ' '.join(new_string)\n",
    "    return temp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Remove punctuation and whitespace\n",
    "itching   ! > itching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text = text.replace('_', ' ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Removing default stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Lemmatization\n",
    "itching > itch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipe(text):\n",
    "    \"\"\"\n",
    "    Combining all preprocessing steps.\n",
    "    \"\"\"\n",
    "    text = text_lowercase(text)\n",
    "    text = convert_number(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_whitespace(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_word(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eng2hi(sentence):\n",
    "    return t.translate(sentence, dest=\"hi\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/english-hindi/cw2_train.csv', index_col=0)\n",
    "test_df = pd.read_csv('../datasets/english-hindi/cw2_dev.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting sent1 to hindi for train and dev datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sent in tqdm(train_df['Sent1'].values[:5000]):\n",
    "    res.append(eng2hi(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in tqdm(train_df['Sent1'].values[5000:]):\n",
    "    res.append(eng2hi(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Sent1'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sent in tqdm(test_df['Sent1'].values):\n",
    "    res.append(eng2hi(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Sent1'] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting dataset to tab spaced strings for ParallelSentencesDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = train_df.iloc[:2300,:]\n",
    "df_2 = train_df.iloc[2300:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1[['Sent2', 'Sent1', 'SimScore']]\n",
    "df_2 = df_2[['Sent2', 'Sent1', 'SimScore']]\n",
    "test_df = test_df[['Sent2', 'Sent1', 'SimScore']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv('../datasets/english-hindi/dev.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.to_csv('../datasets/english-hindi/train.txt', header=None, index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('../datasets/english-hindi/test.txt', header=None, index=False, sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
